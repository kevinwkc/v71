@Proceedings{KDDADF-2017,
    booktitle = {Proceedings of the KDD 2017: Workshop on Anomaly Detection in Finance},
    name = {Knowledge Discovery and Data Mining},
    shortname = {KDD},
    sections = {Preface|Invited Papers|Contributed Papers},
    editor = {Archana Anandakrishnan},
    volume = {71},
    year = {2017},
    start = {2017-08-14},
    end = {2017-08-14},
    url = {https://sites.google.com/view/kdd-adf-2017/call-for-papers},
    location = {Halifax, Nova Scotia, Canada},
    shortname={KDDADF},
	published={2018-01-07}
}
    
@InProceedings{anandakrishnan17,
	title = {Anomaly Detection in Finance: Editors' Introduction},
	author = {Anandakrishnan, Archana and Kumar, Senthil and Statnikov, Alexander and Faruquie, Tanveer and W Xu, Di},
	pages = {1-7},
	year={2017},
	section={Preface},
	abstract = {}
}

@InProceedings{shabat17,
	title = {Uncovering Unknown Unknowns in Financial Services Big Data by Unsupervised Methodologies: Present and Future trends},
	author = {Shabat, Gil and Segev, David and Averbuch, Amir},
	pages = {8-19},
	year={2017},
	section={Invited Papers},
	abstract = {Currently, unknown unknowns in high dimensional big data environments can go unnoticed for a long period of time. The failure to detect anomalies in critical infrastructure data can result in extensive financial, operational, reputational and life threatening consequences. In this paper, we describe algorithms for an automatic and unsupervised anomaly detection that do not necessitate domain expertise, signatures, rules, patterns or semantics understanding of the features. We propose several new methodologies for anomaly detection to protect critical infrastructures, with emphasis on finance, spanning from theory to actionable technology. Although anomalies can originate from several sources, we also show that cyber threat,financial and operational malfunction are converging into a single detection paradigm. Performance comparison between different algorithms (ours and others) is presented as well as examples from real use cases.}
}

@InProceedings{ye17,
	title = {Analytical Techniques for Anomaly Detection Through Features, Signal-Noise Separation and Partial-Value Association},
	author = {Ye, Nong},
	pages = {20-32},
	year={2017},
	section={Invited Papers},
	abstract = {This paper presents three analytical techniques for anomaly detection which can play an important role for anomaly detection in finance: the feature extraction technique, the signal-noise separation technique, and the Partial-Value Association Discovery (PVAD) algorithm. The feature extraction technique emphasizes the importance of extracting various data features which may be better at separating anomalies from norms than using raw data. The signal-noise separation technique considers an anomaly as the signal to detect and the norm as the noise and employs both anomaly models and norm models to detect anomalies accurately. The PVAD algorithm enables learning from data to build anomaly patterns and norm patterns which capture both partial-value and full-value variable relations as well as interactive, concurrent effects of multiple variables.}
}

@InProceedings{kuchar17,
	title = {Spotlighting Anomalies using Frequent Patterns},
	author = {Kuchar, Jaroslav and Svatek, Vojtech},
	pages = {33-42},
	year={2017},
	section={Contributed Papers},
	abstract = {Approaches for anomaly detection based on frequent pattern mining follow the paradigm: if an instance contains more frequent patterns, it means that this data instance is unlikely to be an anomaly. This concept can be used in financial industry to reveal contextual anomalies. The main contribution of this paper is an approach that includes a novel formula for computation of anomaly scores. We evaluated the proposed approach on baseline datasets and present a use case on a real world financial dataset. We also propose a way how to explain the anomaly to the users. Implementations of the evaluated algorithms and experiments are available online in R.}
}

@InProceedings{kashef17,
	title = {Ensemble-Based Anomaly Detetction using Cooperative Learning},
	author = {Kashef, R.F.},
	pages = {43-55},
	year={2017},
	section={Contributed Papers},
	abstract = {Using the same process and functionality to solve both clustering and outlier discovery is highly desired. Such integration will be of great benefit to discover outliers in data and consequently obtain better clustering results after eliminating the set of outliers. It is known that the capability of discovering outliers using clustering-based techniques is mainly based on the quality of the adopted clustering. In this paper, a novel Cooperative Clustering Outlier Detection (CCOD) algorithm is presented. It involves multiple clustering techniques; the goal of the cooperative approach is to discover those outliers that are not detected by the single clustering-based outlier detection approaches using the methodology of cooperation. Undertaken experimental results show that the detection accuracy of the cooperative technique is better than that of the typical clustering-based FindCBLOF method over a number of artificial, gene expression and text document datasets.}
}

@InProceedings{toledano17,
	title = {Real-time anomaly detection system for time series at scale},
	author = {Toledano, Meir and Cohen, Ira and Ben-Simhon, Yonatan and Tadeski, Inbal},
	pages = {56-65},
	year={2017},
	section={Contributed Papers},
	abstract = {This paper describes the design considerations and general outline of an anomaly detection system used by Anodot. We present results of the system on a large set of metrics collected from multiple companies.}
}

@InProceedings{cao17,
	title = {Collective Fraud Detection Capturing Inter-Transaction Dependency},
	author = {Cao, Bokai and Mao, Mia and Viidu, Siim and Yu, Philip},
	pages = {66-75},
	year={2017},
	section={Contributed Papers},	
	abstract = {In e-commerce, different payment transactions have different levels of risk. Risk is generally higher for digital goods, but it also differs based on product and its popularity, the offer type (packaged game, virtual currency to a game or subscription service), storefront and geography. Existing fraud policies and models make decisions independently for each transaction based on transaction attributes, payment velocities, user characteristics, and other relevant information. However, suspicious transactions may still evade detection and hence we propose a novel approach leveraging a graph based perspective to uncover relationships among suspicious transactions, i.e., inter-transaction dependency. Our focus is to detect suspicious transactions by capturing common fraudulent behaviors that would not be considered suspicious when being considered in isolation. In this paper, we present HitFraud that leverages heterogeneous information networks for collective fraud detection by exploring correlated and fast evolving fraudulent behaviors. First, a heterogeneous information network is designed to link entities of interest in the transaction database via different semantics. Then, graph based features are efficiently discovered from the network exploiting the concept of meta-paths, and decisions on frauds are made collectively on test instances. Experiments on real-world payment transaction data from Electronic Arts demonstrate that the prediction performance is eectively boosted by HitFraud where the computation of meta-path based features is largely optimized. Notably, recall can be improved up to 7.93% and F-score 4.62% compared to baselines.}
}   

@InProceedings{ki17,
	title = {PD-FDS: Purchase Density based Online Credit Card Fraud Detection System},
	author = {Ki, Youngjoon and Yoon, Ji Won },
	pages = {76-84},
	year={2017},
	section={Contributed Papers},
	abstract = {Credit card fraud detection is an endless war between fraudsters and payment service providers. Indeed, annual global financial loss by credit card frauds has increased. Fraudsters have been organized and systematized, attempting to nd weak points of existing fraud detection system (FDS). State-of-the-art FDS approaches utilize already existing fraud cases, which can result in dierent FDS by payment service providers. Therefore, a new payment service provider may not have room for installing a FDS due to the lack of fraudulent cases. Moreover, credit card transactions contain the legitimate owner's personal information, which can be exposed to an honest but curious fraud analyst. In this paper, we propose a purchase density based FDS (PD-FDS) that uses three features which are not related to personal information. PD-FDS does not require already existing fraudulent transactions and also shows low false positive rate (<0.01).}
}

@InProceedings{ram17,
	title = {Fraud Detection with Density Estimation Trees},
	author = {Ram, Parikshit and Gray, Alexander G.},
	pages = {85-94},
	year={2017},
	section={Contributed Papers},
	abstract = {We consider the problem of anomaly detection in finance. An application of interest is the detection of first-time fraud where new classes of fraud need to be detected using unsupervised learning to augment the existing supervised learning techniques that capture known classes of frauds. This domain usually has the following requirements - (i) the ability to handle data containing both numerical and categorical features, (ii) very low latency real-time detection, and (iii) interpretability.
	We propose the use of a variant of density estimation trees (DETs) (Ram and Gray, 2011) for anomaly detection using distributional properties of the data. We formally present a procedure for handling data sets with both categorical and numerical features while Ram and Gray (2011) focused mainly on data sets with all numerical features. DETs have demonstrably fast prediction times, orders of magnitude faster than other density estimators like kernel density estimators. The estimation of the density and the anomalousness score for any new item can be done very eciently. Beyond the flexibility and effciency, DETs are also quite interpretable. For the task of anomaly detection, DETs can generate a set of decision rules that lead to high anomalous-ness scores. We empirically demonstrate these capabilities on a publicly available fraud data set.}
}

@InProceedings{love17,
	title = {An Automated System for Data Attribute Anomaly Detection},
	author = {Love, David and Aggarwal, Nalin and Statnikov, Alexander and Yuan, Chao},
	pages = {95-101},
	year={2017},
	section={Contributed Papers},
	abstract = {We introduce DataQC, an automated system for data attribute anomaly detection for the purpose of improving data quality. Large organizations can have non-standardized or inconsistent data quality checking practices being followed across different departments. The key motivation behind the development of such a system is to 1) achieve a standard for anomaly detection 2) facilitate quick identification of obvious anomalies 3) reduce human judgment in data anomaly detection 4) facilitate prompt corrective action by data scientists. Most of the methods and techniques used during the development of this system are well known and have been widely used by finance professionals who deal with data. Our contribution is to provide a system that improves overall effciency, interpretability, and objectivity for detecting data attribute anomalies.}
}


@InProceedings{vanadelsberg17,
	title = {Binned Kernels for Anomaly Detection in Multi-timescale Data using Gaussian Processes},
	author = {van Adelsberg, Matthew and Schwantes, Christian},
	pages = {102-113},
	year={2017},
	section={Contributed Papers},
	abstract = {Financial services and technology companies invest significantly in monitoring their complex technology infrastructures to allow for quick responses to technology failures. Because of the volume and velocity of signals monitored (e.g., customer transaction volume, API calls, server CPU utilization, etc.), they require sophisticated models of normal system behavior to determine when a component falls into an anomalous state. Gaussian processes (GPs) are flexible, Bayesian nonparametric models that have successfully been used for time series forecasting, interpolation, and anomaly detection in complex data sets. Despite the growing use of GPs for time series analysis in the literature, these methods scale poorly with the size of the data. In particular, data sets containing multiple timescales can pose a problem for GPs, as they can require a large number of points for training.
	We describe a novel method for including long and short timescale information without including an impractical number of data points through the use of a binned process, defined as the definite integral over a latent Gaussian process. This results in a binned covariance function for the time series, which we use to t and forecast data at multiple resolutions. The resulting models achieve higher accuracy with fewer data points than their non-binned counterparts, and are more robust to long tailed noise, heteroskedasticity, and data artifacts.}
}

@InProceedings{lasaga17,
	title = {Deep Learning to Detect Medical Treatment Fraud},
	author = {Lasaga, Daniel and Santhana, Prakash},
	pages = {114-120},
	year={2017},
	section={Contributed Papers},
	abstract = {Excessive treatment or testing of patients is considered one of the most ubiquitous and persistent forms of waste and abuse in healthcare. Some estimates show excessive treatment to be as high as 8% of all medical insurance provider expenditures. It is very difficult to identify an extraneous or unnecessary procedure or drug because there is such a wide variety of diagnoses and an equally large number of treatment options.
	Our goal in this paper was to show how RBMs can be utilized effectively to ferret out abnormal treatments where the prescribed treatment for a given diagnosis is not strictly followed. To test our hypothesis we generated 200,000 different injuries and injected 10% of the injuries with unnecessary treatments to reflect estimated industry prevalence levels. Using testing and training sets we found that Restricted Boltzmann Machines (RBMs) were able to reach AUCs of .95, lifts at 9.5 and recalls at 50%. Implementing our approach on real-world client datasets have shown performances levels that approach simulation performances despite additional noise.}
}

@InProceedings{miller17,
	title = {Sleuthing for adverse outcomes: Using anomaly detection to identify unusual behaviors of third-party agents},
	author = {Miller, Michelle and Cezeaux, Robert},
	pages = {121-125},
	year={2017},
	section={Contributed Papers},
	abstract = {Business transactions between customers and financing entities are often governed by intermediary agents. In this scenario, actions taken by these agents can aect the likelihood of adverse outcomes for both the customers and thefinancial institution. Our goal is to establish a general framework that identies these types of anomalous agents. In this paper, we demonstrate a novel application of anomaly detection using isolation forests to identify which agents may be associated with adverse outcomes. We apply a genetic algorithm to understand which features were key to the performance of anomaly detection and and suggest a general framework for problems that similarly concern the behaviors of third-party agents.}
}
